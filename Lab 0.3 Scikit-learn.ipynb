{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to  Machine learning with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [glouppe's tutorial](https://github.com/glouppe/tutorials-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-learn is the most popular machine learning in Python.\n",
    "* Simple and efficient algorithm implementations\n",
    "* Implements a wide variety of well-established machine learning algorithms\n",
    "* Has extensive <a href=\"http://scikit-learn.org/dev/documentation.html\">documentation</a> \n",
    "* Lots of available <a href=\"http://scikit-learn.org/dev/auto_examples/index.html\">examples</a>\n",
    "* Project is mature and stable and follows srict development guidelines \n",
    "* Builds upon NumPy and SciPy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Supervised learning:__\n",
    "\n",
    "* Linear models (Ridge, Lasso, Elastic Net, ...)\n",
    "* Support Vector Machines\n",
    "* Tree-based methods (Random Forests, Bagging, GBRT, ...)\n",
    "* Nearest neighbors \n",
    "* Neural networks (basics)\n",
    "* Gaussian Processes\n",
    "* Feature selection\n",
    "\n",
    "__Unsupervised learning:__\n",
    "\n",
    "* Clustering (KMeans, Ward, ...)\n",
    "* Matrix decomposition (PCA, ICA, ...)\n",
    "* Density estimation\n",
    "* Outlier detection\n",
    "\n",
    "__Model selection and evaluation:__\n",
    "\n",
    "* Cross-validation\n",
    "* Grid-search\n",
    "* Lots of metrics\n",
    "\n",
    "_... and more!_ (See [Reference](http://scikit-learn.org/dev/modules/classes.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "\n",
    "\n",
    "All learning algorithms in scikit-learn share a uniform and limited API consisting of complementary interfaces:\n",
    "\n",
    "- an `estimator` interface for building and fitting models;\n",
    "- a `predictor` interface for making predictions;\n",
    "- a `transformer` interface for converting data.\n",
    "\n",
    "Goal: enforce a simple and consistent API to __make it trivial to swap or plug algorithms__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators\n",
    "\n",
    "The basic abstraction in sklearn is an estimator. Estimators implement the `fit(X, y=None)` method that fits the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors\n",
    "\n",
    "Predictors use a fitted model to make predictions on data. They implement the `predict(X)` method that returns the predicted `y` values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Transformers change the data to new representations. They implement the `transform(X, y=None)` method that returns the modified data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Classification Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes as a finite learning set ${\\cal L} = (X, y)$ where\n",
    "* Input samples are given as an array $X$ of shape `n_samples` $\\times$ `n_features`, taking their values in ${\\cal X}$;\n",
    "* Output values are given as an array $y$, taking _symbolic_ values in ${\\cal Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of supervised classification is to build an estimator $\\varphi: {\\cal X} \\mapsto {\\cal Y}$ minimizing\n",
    "\n",
    "$$\n",
    "Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}\n",
    "$$\n",
    "\n",
    "where $\\ell$ is a loss function, e.g., the zero-one loss for classification $\\ell_{01}(Y,\\hat{Y}) = 1(Y \\neq \\hat{Y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data loading and inspection\n",
    "\n",
    "- Input data => Numpy arrays or Scipy sparse matrices ;\n",
    "- $X$ => Data samples. Shape  => `n_samples` $\\times$ `n_features`\n",
    "- $y$ => Labels. Shape => `n_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pprint\n",
    "\n",
    "bc_data = load_breast_cancer()\n",
    "X, y = bc_data.data, bc_data.target\n",
    "\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "print(\"X shape: (n_samples) {} x (n_features) {}\"\n",
    "      .format(X.shape[0], X.shape[1]))\n",
    "print(\"y shape: (n_samples) {}\".format(y.shape[0]))\n",
    "\n",
    "# set converts a list into a set containing the unique elements in the list\n",
    "print(\"Unique labels: {}\".format(set(y)))\n",
    "print(\"Unique labels: {}\".format(bc_data.target_names))\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "print(\"Features:\")\n",
    "pprint.pprint(bc_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice way to gather intuition about the data is to plot the features against each other.\n",
    "\n",
    "This shows how good we can separate the classes based on the features\n",
    "\n",
    "First let's define some plotting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_features(X, feature_idx, feature_names, labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    zeros = ax.scatter(\n",
    "        X[y == 0, feature_idx[0]],\n",
    "        X[y == 0, feature_idx[1]], \n",
    "        label=labels[0], c='orange', alpha=0.3, s=100)\n",
    "    ones = ax.scatter(\n",
    "        X[y == 1, feature_idx[0]], \n",
    "        X[y == 1, feature_idx[1]], \n",
    "        label=labels[1], c='b', alpha=0.3, s=100)\n",
    "    plt.title(\"{} vs {}\".format(feature_names[0], feature_names[1]))\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the features. Note how mean radius and mean perimeter are linearly dependent. Is this a good thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(\n",
    "    X, [0, 2],\n",
    "    [bc_data.feature_names[0], bc_data.feature_names[2]],\n",
    "    bc_data.target_names)\n",
    "plot_features(\n",
    "    X, [1, 2],\n",
    "    [bc_data.feature_names[1], bc_data.feature_names[2]],\n",
    "    bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Transformers to process our data before classifying them to obtain more convenient representations.\n",
    "\n",
    "A widely used preprocessing step is dimensionality reduction through Principal Component Analysis (PCA). You will learn everything about PCA during the semester.\n",
    "\n",
    "PCA is useful for visualization or for selecting latent features for classification.\n",
    "\n",
    "In sklearn, PCA is implemented as a Estimator / Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "plot_features(\n",
    "    X_pca, [0, 1], \n",
    "    ['principal component 1', 'principal component 2'], \n",
    "    bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common preprossecing Transformer is the Standard Scaler which normalizes data to have zero mean and unit std using the transformation\n",
    "\n",
    "$$\n",
    "x_{new} = \\frac{x_{old} - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_new = StandardScaler().fit_transform(X_pca)\n",
    "\n",
    "plot_features(\n",
    "    X_new, [0, 1],\n",
    "    ['scaled principal component 2', 'scaled principal component 2'], \n",
    "    bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define some utilities to visualize the classifier's __decision function__.\n",
    "\n",
    "This shows us where the classifier decides to separate the data into the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_meshgrid(x, y, h=.1):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_clf(clf, X, y, labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    # title for the plots\n",
    "    title = ('Decision surface of Classifier')\n",
    "    # Set-up grid for plotting.\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    zeros = ax.scatter(\n",
    "        X0[y == 0], X1[y == 0],\n",
    "        c='blue', label=labels[0],\n",
    "        s=60, alpha=0.9, edgecolors='k')\n",
    "    ones = ax.scatter(\n",
    "        X0[y == 1], X1[y == 1],\n",
    "        c='red', label=labels[1], \n",
    "        s=60, alpha=0.9, edgecolors='k')\n",
    "    \n",
    "    ax.set_ylabel(labels[1])\n",
    "    ax.set_xlabel(labels[0])\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit a Logistic Regression (LR) model to classify the data in the 2 classes. LR tries to model the decision boundary as an hyperplane.\n",
    "\n",
    "LR implements the Estimator / Predictor methods\n",
    "\n",
    "Observe LR tries to separate data by drawing a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_new[17:27])\n",
    "\n",
    "y_true = y[17:27]\n",
    "\n",
    "print(\"LR - Predicted labels:\\t {}\".format(y_pred))\n",
    "print(\"True labels:\\t\\t {}\".format(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commonly used classifier is Support Vector Machines (SVM) that tries to find the hyperplane which has the largest distance to the nearest training points of any class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\")  # try kernel=\"rbf\" instead\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR and SVM both try to separate data by drawing a line / hyperplane. Such classifiers are called `linear classifiers`.\n",
    "\n",
    "We can modify SVM to first project the data using a `kernel`. You will learn about this during the semester.\n",
    "\n",
    "The important thing here is that with kernels we can get non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"rbf\")  # try kernel=\"rbf\" instead\n",
    "clf.fit(X_new, y)\n",
    "plot_clf(clf, X_new, y, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that we want to learn an estimator $\\varphi$ minimizing the generalization error $Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}$.\n",
    "\n",
    "- Problem: Since $P_{X,Y}$ is unknown, the generalization error $Err(\\varphi)$ cannot be evaluated.\n",
    "\n",
    "- Solution: Use a proxy to approximate $Err(\\varphi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error on training data \n",
    "\n",
    "A common mistake is to calculate classification error on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import zero_one_loss\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_new, y)\n",
    "print(\"Training error =\", zero_one_loss(y, clf.predict(X_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error on test data\n",
    "\n",
    "Issue: the training error is a __biased__ estimate of the generalization error.\n",
    "\n",
    "Solution: Divide ${\\cal L}$ into two disjoint parts called training and test sets (usually using 70% for training and 30% for test).\n",
    "- Use the training set for fitting the model;\n",
    "- Use the test set for evaluation only, thereby yielding an unbiased estimate.\n",
    "\n",
    "The moded should never see samples from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, random_state=2)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training error =\", zero_one_loss(y_train, clf.predict(X_train)))\n",
    "print(\"Test error =\", zero_one_loss(y_test, clf.predict(X_test)))\n",
    "\n",
    "plot_clf(clf, X_test, y_test, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: Beware of bias when you estimate model performance:\n",
    "- Training score is often an optimistic estimate of the true performance;\n",
    "- __The same data should not be used both for training and evaluation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: \n",
    "- When ${\\cal L}$ is small, training on 70% of the data may lead to a model that is significantly different from a model that would have been learned on the entire set ${\\cal L}$. \n",
    "- Yet, increasing the size of the training set (resp. decreasing the size of the test set), might lead to an inaccurate estimate of the generalization error. \n",
    "\n",
    "__Hint__: Try to change the `random_state` in the above example and observe the change in the test error and the decision function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: K-Fold cross-validation. \n",
    "- Split ${\\cal L}$ into K small disjoint folds. \n",
    "- Train on K-1 folds, evaluate the test error one the held-out fold.\n",
    "- Repeat for all combinations and average the K estimates of the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), X_new, y, \n",
    "                         cv=KFold(n_splits=5, random_state=42), \n",
    "                         scoring=\"accuracy\")\n",
    "print(\"CV error = %f +-%f\" % (1. - np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the `random_state` in the above example and observe the CV error. \n",
    "\n",
    "We can be a lot more confident in the CV error estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy score__\n",
    "\n",
    "When evaluating the models we want to report some standard metrics.\n",
    "\n",
    "The simplest metric for classification is the __accuracy score__, which is the default metric in most models.\n",
    "\n",
    "The accuracy score is simply the percentage of the correct predictions.\n",
    "\n",
    "In the case of binary classification is simply $1 - \\ell_{01}(Y,\\hat{Y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score is classifiers default score. clf.score: {}\"\n",
    "      .format(clf.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / y_true.shape[0]\n",
    "\n",
    "print(\"We can implement it very easily. acc: {}\"\n",
    "      .format(acc(y_test, clf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"But it's already implemented in sklearn. accuracy_score: {}\"\n",
    "      .format(accuracy_score(y_test, clf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Precision, recall and F1 score__\n",
    "\n",
    "In many cases accuracy is simply not enough to evaluate a classifier. Instead we may need to know how many samples get incorrectly classified as positive (the False Positives $FP$) and how many get classified incorrectly as negative (the False Negatives $FN$).\n",
    "\n",
    "For example in the malignant vs benign breast cancer we are studying, the consequences are far larger if we missclassify a malignant tumor as benign than if we missclassify a benign tumor as malignant.\n",
    "\n",
    "The precision, recall and F1 score are metrics that indicate the ratios of False Positives (FP), False Negatives (FN), and True Positive (TP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Precision = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F1 = \\frac{2 * Precision * Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Precision =\", precision_score(y_test, clf.predict(X_test)))\n",
    "print(\"Recall =\", recall_score(y_test, clf.predict(X_test)))\n",
    "print(\"F1 =\", f1_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "An even more fine grained metric is the confusion matrix, which shows the number of samples that get missclassified in each class.\n",
    "\n",
    "Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).[2] The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many points to be covered for model selection, in this tutorial we will consider the __hyperparameter search__.\n",
    "\n",
    "Hyperparameter search is the problem of finding the best parameters for the model to fit the best performing model.\n",
    "\n",
    "The simplest solution is to create a grid of all possible parameters and evaluate the model with each set of parameters in the grid, i.e. to perform a grid search.\n",
    "\n",
    "Sklearn implements grid search as an Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Note in param_grid we pass the names of the parameters we want to search\n",
    "# as they appear in the classifier's class arguments and the possible values\n",
    "# they can take\n",
    "grid = GridSearchCV(LogisticRegression(),\n",
    "                    param_grid={\"C\": np.linspace(0.05, 5, 50),\n",
    "                                \"penalty\": [\"l1\", \"l2\"]},\n",
    "                    scoring=\"f1\",\n",
    "                    cv=5, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_new, y)\n",
    "\n",
    "print(\"Best score = {}, Best parameters = {}\".format(grid.best_score_, \n",
    "                                                     grid.best_params_))\n",
    "\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Should you report the best score as an estimate of the generalization error of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: No\n",
    "\n",
    "- `grid.best_score_` is not independent from the best model, since its construction was guided by the optimization of this quantity. \n",
    "\n",
    "- As a result, the optimized `grid.best_score_` estimate _may_ in fact be a biased, optimistic, estimate of the true performance of the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__: Use __nested__ cross-validation for correctly selecting the model __and__ correctly evaluating its performance (or use a validation set if you have a lot of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(),\n",
    "                    param_grid={\"C\": np.linspace(0.05, 5, 50), \n",
    "                                \"penalty\": [\"l1\", \"l2\"]},\n",
    "                    scoring=\"f1\",\n",
    "                    cv=5, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(grid, X_new, y, cv=5, scoring=\"f1\")\n",
    "\n",
    "# Unbiased estimate of the accuracy\n",
    "print(\"{} +-{}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(**best_params).fit(X_train, y_train)\n",
    "plot_clf(clf, X_test, y_test, bc_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn introduces the concept of pipelines. Pipelines not only make the code short and easy to understand, but also make it easy to ensure that everything runs on the correct set of data.\n",
    "\n",
    "For example in the above code we have a serious bug. When we preprocessed PCA and StandardScaler, we used the entire dataset to fit them, not the training dataset.\n",
    "\n",
    "__Incorrect preprocessing is another way to introduce bias to the models__\n",
    "\n",
    "or better\n",
    "\n",
    "__Incorrect preprocessing is the most common way to introduce bias to the models__\n",
    "\n",
    "or even better\n",
    "\n",
    "__Incorrect preprocessing is the hardest to spot source of bias for the models__\n",
    "\n",
    "\n",
    "A correct pipeline should look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "bc_data = load_breast_cancer()\n",
    "X, y = bc_data.data, bc_data.target\n",
    "\n",
    "\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('pca', PCA()),\n",
    "    ('znorm', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [2, 10, 20],\n",
    "    'lr__C': np.linspace(0.05, 5, 10),\n",
    "    'lr__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    clf_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_params = grid_search.fit(X, y).best_params_\n",
    "\n",
    "best_scores = cross_val_score(grid_search,\n",
    "                              X, y, \n",
    "                              cv=5, \n",
    "                              scoring=\"f1\")\n",
    "import pprint\n",
    "print(\"Best model params:\")\n",
    "pprint.pprint(best_params)\n",
    "print(\"Best score: {}+-{}\".format(np.mean(best_scores), \n",
    "                                  np.std(best_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
